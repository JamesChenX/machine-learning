
笔记的方向：数学 + 深度学习。把这些基础学牢，看各种论文就比较轻松。

机器学习领域的知识学得快（学会+实操。强调“实操”是因为机器学习知识的实操相对比较简单，不像视唱练耳，比如相对音高的概念非常简单，但是一些人死活也听不出相对音高），但忘得也快。

而如果有笔记记录，可以快速refresh各种知识 + 学习过程中遇到的坑，避免反复掉坑。
# 数学

工具
* 公式可视化：https://www.wolframalpha.com/input?i2d=true&i=2x&lang=zh
* 数学公式识别（如识`以LaTeX文本形式输出附件中的公式`）+求解：
	* deepseek或其他GPT
	* https://notegpt.io/ai-math-solver
* 数学学习“词典”：https://www.mathsisfun.com/calculus/

学习建议：

1. 初学ML时，通常不用特意宽泛地学数学，而是学ML的时候，遇到相关数学知识，再专门学其相关的数学知识，这样带着问题去学，这样学习更有趣+更记得牢+能知道怎么把数学知识用到实处。
2. ML基础学完后，如果有兴趣，可以再找专门讲解ML中的数学的书来看。加深对ML背后数学运用的理解。
3. 另外，学习的过程中，如果搞不明白一些数学细节（尤其是某某代码为什么这么写，其背后的数学原理是啥？），则问问LLM。非常好用。

# 深度学习基础

## 人工神经网络基础（Artificial Neural Network (ANN)）

### 生物学中的神经网络

大脑的神经元​
1. 神经元形成网络。
2. 对于从其他多个神经元传递过来的信号，
	1. 如果它们的和不超过某个固定大小的值（阈值）​，则神经元不做出任何反应。
	   对于生命来说，神经元忽略微小的输入信号，这是十分重要的。反之，如果神经元对于任何微小的信号都变得兴奋，神经系统就将不稳定​。
	2. 如果它们的和超过某个固定大小的值（阈值）​，则神经元做出反应（称为点火）​，向另外的神经元传递固定强度的信号。
3. 从多个神经元传递过来的信号之和中，每个信号对应的权重不一样。

### 人工神经网络（ANN）

是一种监督学习算法，其灵感来自人类大脑的运作方式。类似于神经元在人类大脑中的连接和激活方式，神经网络接受输入并将其传递给一个函数，导致某些后续神经元被激活，从而产生输出。

简单来说，**它可以被看作是一个数学函数，将一个或多个张量（Tensor）作为输入，预测一个或多个张量为输出**。

![[1_BIpRgx5FsEMhr1k2EqBKFg.gif]]

作用：
* 通用逼近定理（Universal approximation theorem）表明，我们总能找到一个足够大的神经网络架构，具有合适的权值，可以精确预测任何给定输入的任何输出。这意味着，对于给定的数据集/任务，我们可以创建一个架构，并不断调整其权值，直到 ANN 预测出我们想要的结果。
* 图像分类
* 生成式人工智能（GenAI）
* 等等

训练神经网络（training the neural network）：调整权值，直到 ANN 学习给定任务的过程

#### 组成部分（极为重要）

神经网络是由**节点（张量，tensor）** 组成的集合，其中相连的节点之间有一个浮点数称为**权重（Weight）**，并且节点以图的形式互连，以一种由**神经网络架构（Architecture**。用于排列输入与输出连接起来的运算）指定的格式返回输出。

1. **输入层（Input Layers）**：这些层将独立变量作为输入。
2. **隐藏层/中间层（Hidden (intermediate) layers）**：这些层连接输入层和输出层，同时在输入数据上执行转换。此外，隐藏层包含节点​，将输入值修改为高维/低维值（higher-/lower-dimensional values）。通过使用各种激活函数（activation functions）来修改中间层节点的值，实现更复杂的表示功能。
   深度学习中的“深度”（Deep）指的就是更大的隐藏层数。
3. **输出层（Output Layer）**：当输入变量通过网络时，该层生成期望的结果值。

前一层的输出作为下一层的输入，直到输出层。

![[Pasted image 20250405163206.png]]
输出层中的节点的数量取决于我们是否在预测连续变量还是分类变量。
如果输出是连续变量，则输出有一个节点。
如果输出是具有m个可能类的分类变量，则输出层中有m个节点。

##### 单个神经元处的输入转换（极为重要）

$$
\begin{align*}
a = f\left(w_0 + \sum_{i=1}^n w_i x_i\right)
\end{align*}
$$

该方程表示通过对包括偏差项在内的输入进行**加权和**并将其传递给**激活函数**来计算神经元激活的过程。

- a是输出值
- w0是偏置项（Bias）。
- wi是权重。
- xi是输入特征。
- f是激活函数。
![[Pasted image 20250405163508.png]]

X1，X2，...，Xn是输入变量，W0是偏置项（Bias）。

![[Pasted image 20250405172238.png]]

##### 激活函数（Activation Function）

![[Pasted image 20250405172611.png]]

![[68747470733a2f2f676174746f6e7765622e756b792e6564752f666163756c74792f6c69756d2f6d6c2f72656c757369676d6f69642e676966.gif]]
（注意结合上述的节点输出值的公式来运用激活函数）

作用：现实世界的数据（如图像、语音、文本）通常是非线性的，而激活函数（Activation Function）在神经网络中引入非线性，使模型能够学习这些复杂模式。如果没有激活函数，神经网络将退化为线性模型，无论有多少层，其表达能力都会受到极大限制。

具体来说

* 避免退化线性模型：无激活函数时，多层网络等价于单层，失去深度学习的优势。
* 逼近复杂函数，增强表达能力：非线性激活函数使网络能够拟合任意连续函数（万能逼近定理）。
* 分层特征学习：不同层通过非线性组合逐步抽象高级特征。
	* 浅层学习低级特征（如边缘、纹理）。
	* 深层组合低级特征为高级特征（如物体、语义）。
* 优化梯度：选择合适的激活函数（如 ReLU）能优化梯度流动，缓解梯度消失/爆炸问题，加速训练。

对比表：

| **激活函数**                  | **数学公式**                             | **输出范围**   | 代码实现                                                              | **优点**       | **缺点**            | **适用场景**         |
| ------------------------- | ------------------------------------ | ---------- | ----------------------------------------------------------------- | ------------ | ----------------- | ---------------- |
| **Sigmoid（Logistic 函数）**  | ![[Pasted image 20250405172839.png]] | (0,1)(0,1) |                                                                   | 平滑，适合概率输出    | 梯度消失，计算成本高        | 二分类输出层           |
| **Tanh（双曲正切函数）**          | ![[Pasted image 20250405172846.png]] | (−1,1)     | def tanh(x): return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) | 输出以 0 为中心    | 梯度消失问题            | 隐藏层              |
| **ReLU（修正线性单元）**          | max⁡(0,x)                            | [0,+∞)     |                                                                   | 计算快，缓解梯度消失   | 神经元可能“死亡”（输出恒为 0） | 隐藏层（最常用）         |
| **Leaky ReLU（带泄漏的 ReLU）** | ![[Pasted image 20250405172903.png]] | (−∞,+∞)    |                                                                   | 缓解 ReLU 死亡问题 | 需调参 αα            | 隐藏层（改进 ReLU）     |
| **Softmax（多分类归一化函数）**     | ![[Pasted image 20250405172912.png]] | (0,1)      | def softmax(x): return np.exp(x)/np.sum(np.exp(x))                | 多分类概率归一化     | 仅用于输出层            | 多分类输出层           |
| **Swish（自门控激活函数）**        | ![[Pasted image 20250405172918.png]] | (−∞,+∞)    |                                                                   | 平滑，优于 ReLU   | 计算稍复杂             | 隐藏层（Google 提出）   |
| **GELU（高斯误差线性单元）**        | ![[Pasted image 20250405172924.png]] | (−∞,+∞)    |                                                                   | 类似 ReLU，但更平滑 | 计算复杂              | Transformer/BERT |

如何选择激活函数：
- **隐藏层**：优先使用 **ReLU**（计算快），可尝试 **Leaky ReLU** 或 **Swish**。
- **二分类输出层**：**Sigmoid**。
- **多分类输出层**：**Softmax**。
- **需要平滑梯度时**：**Tanh** 或 **GELU**（如自然语言处理模型）。

##### 成本函数（Cost functions）

作用：
* 用于计算损失值（Loss Values），即预测值与实际值之间的误差值。该误差值会在之后的反向传播过程中，用于找到损失为零或接近零的最佳权重。

###### 计算方式

连续变量（Continuous variable）预测中的损失计算：

使用均方误差（Mean squared error）损失函数计算：
  $$\begin{align*}
f_\theta &= \frac{1}{m}\sum_{i=1}^m (\mathbf{y}_i - \hat{\mathbf{y}}_i)^2 \\
\hat{\mathbf{y}}_i &= \eta_\theta(\mathbf{x}_i)
\end{align*}$$

![[Pasted image 20250405174451.png]]

m是数据量

```python
def mse(p, y):
    return np.mean(np.square(p - y))
```

使用平均绝对误差（Mean Absolute Error）

```python
def mae(p, y):
    return np.mean(np.abs(p-y))
```

---

分类变量（Categorical variable）预测中的损失计算：

当要预测的变量是离散的（即变量中只有几个类别）​，我们通常使用分类交叉熵损失函数（cross-entropy loss function）。

	  $$
- \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^m y_i \log(p_i)
$$

![[Pasted image 20250405175303.png]]

当正确类的概率高时，损失值较小。概率范围在0到1之间。因此，当概率为1时，最小可能损失为0，当概率为0时，最大损失为无穷大。

```python
def categorical_cross_entropy(p, y):
    return -np.mean(np.log(p[np.arange(len(y)),y]))
```

当要预测的变量有两个不同的值时，损失函数是二进制交叉熵（binary cross-entropy loss function）。

$$
\frac{1}{m} \sum_{i=1}^m \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)
$$

![[Pasted image 20250405175111.png]]

```python
def binary_cross_entropy(p, y):
    return -np.mean((y*np.log(p)+(1-y)*np.log(1-p)))
```
	
#### 前馈传播（Feedforward Propagation）与反向传播（Backpropagation）（极为重要）

**前馈传播：数据通过网络逐层前向传播，计算每一层的输出，直到得到最终的预测值和损失函数（Loss）**
**反向传播：从输出层开始，基于误差调整权重，逐层反向计算梯度，利用链式法则将梯度分解为局部梯度的乘积**
**反复迭代（反复进行前馈传播+反向传播）直至模型收敛。每论迭代被称作周期（epoch）**

| **对比项**    | **前馈传播**   | **反向传播**    |
| ---------- | ---------- | ----------- |
| **方向**     | 输入层 → 输出层  | 输出层 → 输入层   |
| **作用/目的**  | 计算预测值      | 计算梯度并更新权重   |
| **计算内容**   | 加权和 + 激活函数 | 误差传播 + 链式求导 |
| **是否需要标签** | 不需要（仅前向计算） | 需要（依赖损失函数）  |

![[1_VF9xl3cZr2_qyoLfDJajZw.gif]]

![[68747470733a2f2f676174746f6e7765622e756b792e6564752f666163756c74792f6c69756d2f6d6c2f6e6e2e676966.gif]]


##### 坡度

**深度学习中的`坡度`指的是：损失变化率，即相对于预测的输出值，损失的坡度（Slope或Gradient）**
**深度学习中的`坡度`指的是：损失变化率，即相对于预测的输出值，损失的坡度（Slope或Gradient）**
**深度学习中的`坡度`指的是：损失变化率，即相对于预测的输出值，损失的坡度（Slope或Gradient）**

**为什么是相对于预测的输出值？因为这样就能反推出需要增大或缩小预测的输出值以减少损失值，进而反推出要增大或缩小权重。**

**坡度的两个重要属性：**
- **方向：告诉预测值应该调大还是调小（比如梯度是+1 → 要调大输出）。**
	- **梯度是正 → 往“增加预测值”方向走。**
	- **梯度是负 → 往“减少预测值”方向走。**
- **大小：告诉调整的力度（比如梯度是2比1的调整力度更大）。**

实际计算（以MSE损失为例）

![[Pasted image 20250405190710.png]]
##### 通俗讲解版

比喻：快递配送站
想象你开了一家快递公司，货物从总部（输入层）出发，经过多个配送站（隐藏层），最后送到客户（输出层）。但有一天客户投诉说货物损坏了（损失函数值高），你得找出哪个配送站搞砸了，并惩罚它（调整参数）。

关键问题：
怎么知道是哪个配送站（哪一层）的责任？
答案：反向调查，从客户开始，一层层往回问！

---

**链式法则：责任传递规则**

链式法则就是**责任划分的数学规则**：

如果A的责任影响了B，B的责任又影响了C，那么A对C的责任 = （A对B的责任 × B对C的责任）。

例子：

儿子考试不及格（损失）是因为作业没写好（最后一层），

作业没写好是因为爸爸没辅导（前一层），

爸爸没辅导是因为加班（更前一层）。
那么公司对儿子不及格的责任 = 公司让爸爸加班的责任 × 爸爸没辅导的责任 × 儿子作业差的责任。

---

反向传播的步骤
场景：一个简单神经网络（输入 → 隐藏层 → 输出 → 损失）
前向传播（送货）：

输入数据（比如“猫的图片”）→ 隐藏层计算 → 输出层预测（比如“70%是猫”）→ 计算损失（比如“真实标签是猫，损失=0.3”）。

反向传播（追责）：

第1步：问输出层——“损失0.3是不是你的错？”

输出层说：“部分是我的错，但我也是被隐藏层坑了！我的责任是 ∂Loss/∂输出 = 0.3”（损失对输出的梯度）。

第2步：问隐藏层——“输出层说是你坑了它，你怎么解释？”

隐藏层说：“我的责任 = （输出层的责任 × 我的激活函数的导数）”（链式法则：梯度相乘）。

第3步：更新参数——“根据责任大小，调整你们的工作（权重）！”

如果某层责任大（梯度大），就多调整它的权重；责任小就少调整。

---

通俗版计算公式
输出层责任：
损失对输出的梯度 = (预测值 - 真实值)
（比如预测0.7，真实1.0，责任= -0.3）

隐藏层责任：
损失对隐藏层的梯度 = 输出层的责任 × 隐藏层激活的导数
（比如输出层责任=-0.3，Sigmoid导数=0.25，那么隐藏层责任= -0.3×0.25 = -0.075）

更新权重：
新权重 = 旧权重 - 学习率 × 责任 × 输入值
（比如隐藏层责任=-0.075，输入值是0.5，学习率0.1，则调整量= -0.1 × -0.075 × 0.5 = +0.00375）

5. 关键点总结
链式法则：像追责一样，从后往前乘梯度。

反向传播：从损失开始，逐层计算“谁该背锅”。

参数更新：根据责任大小（梯度）调整权重。

##### 数学基础

* 导数：用于计算相对于权重，损失的坡度（Slope或Gradient）。因此，如果坡度越大，说明损失变化率越大。
  换言之，**坡度下降（Gradient Descent）指的就是在执行反向传播逻辑的过程中，让损失率（坡度）下降的过程，而随机坡度下降（Stochastic gradient descent）中的“随机”指的是使用随机的样本**。
  https://www.mathsisfun.com/calculus/derivatives-introduction.html
  ![[Pasted image 20250405183434.png]]
* 导数的链式法则的运用：
  * 通用步骤：
    * 计算当前层的输出对输入的梯度（局部梯度）。
    * 接收来自上一层的梯度（上游梯度）。
    * 将两者相乘，得到当前层的输入梯度。
    * 继续反向传递梯度，并计算参数梯度。
  对应上述例子中的：公司对儿子不及格的责任 = 公司让爸爸加班的责任 × 爸爸没辅导的责任 × 儿子作业差的责任
  ![[Pasted image 20250405184946.png]]
* 偏导数：


代码实现

```python
import numpy as np  
from copy import deepcopy  
import matplotlib.pyplot as plt  
  
  
def feed_forward(inputs, outputs, weights):  
    # inputs * 权重 + 偏移值  
    pre_hidden = np.dot(inputs, weights[0]) + weights[1]  
    # 应用sigmoid激活函数，计算隐藏层节点值  
    hidden = 1 / (1 + np.exp(-pre_hidden))  
    out = np.dot(hidden, weights[2]) + weights[3]  
    mean_squared_error = np.mean(np.square(out - outputs))  
    return mean_squared_error  
  
  
def update_weights(inputs, outputs, weights, lr):  
    """  
    过程：前馈传播 + 反向传播  
    """    original_weights = deepcopy(weights)  
    updated_weights = deepcopy(weights)  
    # 通过前馈传播，计算出初始loss，用于后面根据该loss做优化  
    original_loss = feed_forward(inputs, outputs, original_weights)  
  
    for i, layer in enumerate(original_weights):  
        for j, weight in np.ndenumerate(layer):  
            temp_weights = deepcopy(weights)  
            # 微调权重值，计算该权重下，loss的梯度  
            temp_weights[i][j] += 0.0001  
            _loss_plus = feed_forward(inputs, outputs, temp_weights)  
            # 梯度（grad） 是loss的导数，指向函数值（这里是误差 loss）增长最快的方向。  
            # 我们的目标是最小化误差，因此需要沿着梯度的反方向调整权重。  
            grad = (_loss_plus - original_loss) / 0.0001  
            # 梯度下降的具体实现，通过沿负梯度方向微调权重，逐步逼近最优解。  
            # 梯度的方向性：  
            # * 如果 grad > 0，说明增加权重会增大误差，因此需要减小权重（-= grad * lr）。  
            # * 如果 grad < 0，说明增加权重会减小误差，因此需要增大权重（-= 负梯度相当于 += 正数）。
            updated_weights[i][j] -= grad * lr  
    return updated_weights, original_loss  
  
  
if __name__ == '__main__':  
    initial_inputs = np.array([[1, 1]])  
    expected_outputs = np.array([[0]])  
    # 随机初始化权重与偏移值  
    weights_and_biases = [  
        # 输入层到隐藏层的权重  
        np.array([[-0.0053, 0.3793],  
                  [-0.5820, -0.5204],  
                  [-0.2723, 0.1896]], dtype=np.float32).T,  
        # 输入层到隐藏层的偏移值  
        np.array([-0.0140, 0.5607, -0.0628], dtype=np.float32),  
        # 隐藏层到输出层的权重  
        np.array([[ 0.1528, -0.1745, -0.1135]], dtype=np.float32).T,  
        # 隐藏层到输出层的偏移值  
        np.array([-0.5516], dtype=np.float32)  
    ]  
    losses = []
    # 训练1000个周期
    for epoch in range(1000):  
        weights_and_biases, loss = update_weights(initial_inputs, expected_outputs, weights_and_biases, 0.01)  
        losses.append(loss)  
    plt.plot(losses)  
    plt.title('Loss over increasing number of epochs')
    plt.show()
```

###### 学习率（Learning Rate）

控制更新的步长，避免因梯度太大而“跨过”最优值。  
如果学习率太大：可能震荡或发散。
如果学习率太小：收敛过慢。  

![[68747470733a2f2f676174746f6e7765622e756b792e6564752f666163756c74792f6c69756d2f6d6c2f6c61726765746f736d616c6c2e676966.gif]]
# 软件工程